1. 神经网络的超参如何调

lr就是超参数，影响速度和收敛结果，一般设置的表现都还算正常，没有这个经验去设置，具体问题具体分析，一般都不断去试

2. 神经网络的层数如何设计? 如何评价一个训练得出的神经网络效果?以及神经网络的优缺点

一般层数：3层，从低往高去试，层数越多，函数越复杂
是不是层数越多越好：答案：当然，这个也是一个研究方向
那是不是直接设置1000层，问题就是此时优化器不好优化，梯度消失，梯度爆炸的问题，模型参数不好学习（收敛）


评价：模型在验证集和测试集上面的效果

优缺点：nn和ML 
nn:不需要手动构建特征，适合处理文本（NLP）,图像（CV），需要数据量特别大（10w+）
ML:适合机器学习模型

3. 梯度消失和爆炸会有什么影响? 如何从数据集的角度避免 (除了LSTM,ReLU)

梯度消失：梯度为0；
$w = w - lr \times \frac{\partial w}{\partial x}$

模型做深，避免梯度消失和爆炸

4. 神经元的输入是只能是0，1吗
当然不是，理论来说：$(-\infty,\infty)$

实际上就是做归一化处理，这样模型学习，优化起来非常好

5. 经过sigmoid激活处理的神经元输出0代表神经元抑制，1代表兴奋,那经过神经网络的输出，如何得出近似概率预测值及损失?

