
人工智能  机器学习阶段 考试题

====选择题=====(每题2分)

1. 下列选项中，关于Matplotlib库说法不正确是。（A ）
A. Matplotlib是一个Python 3D绘图库
B. 可输出PNG、PDF等格式
C. 渐进、交互的方式实现数据可视化
D. 使用简单

2. 下列函数中，可以绘制散点图的函数是。（ B）
A. hist()
B. scatter()
C. bar()
D. pie()

3. 关于Pandas中数据排序，下列说法正确的是（ A ）。
A.即可以按照行索引排序，也可以按照列索引排序
B.sort_index()方法表示按照值进行排序
C.sort_values()方法表示按照索引进行排序
D.默认情况下，sort_index()方法按照降序排列

4. 下列关于kd-tree的说法错误的是：（ C）
A. 它是基于欧氏距离来度量样本之间的距离的
B. 它是一个二叉树
C. 在选取根节点时使用的是方差较小的那一维数据
D. 每个节点的左子节点在当前分割维度上的数据都比右子节点小

5. 关于特征预处理，下列说法中错误的是（B ）
A. 包含标准化和归一化 
B. 标准化在任何场景下受异常值的影响都很小
C. 归一化利用了样本中的最大值和最小值 
D. 标准化实际上是将数据在样本的标准差上做了等比例的缩放操作

6. 关于交叉验证，下列说法中错误的是（A ）
A. 交叉验证能够提升模型的准确率 
B. 交叉验证能够让样本数据被模型充分利用
C. 交叉验证搭配网格搜索能够提升我们查找最优超参数组合的效率
D. 使用网格搜索时我们一般会提供超参数的可能取值字典

7. 下列关于线性回归的说法错误的是：（B）
A. 它是使用回归分析的统计学习模型来研究变量之间可能存在的关系
B. 它只能用于研究变量之间属于线性关系的场景
C. 寻找最优模型时可以通过正规方程或者梯度下降的方法进行参数优化
D. 单纯的线性回归模型比较容易出现过拟合的现象

8. 关于梯度下降法的描述，错误的是：（C）
A. 随机梯度下降法是每次使用一个样本的数据来迭代权重
B. 全梯度下降法的计算量随着样本数量的增加而增加
C. 随机平均梯度下降法不依赖与已经计算过的梯度
D. 小批量随机梯度下降法综合了FGD和SGD的优势

9. 下面关于随机森林和GBDT的说法正确的是：（D）
① 这两种方法都可以用来解决分类问题
② 随机森林解决分类问题，GBDT解决回归问题
③ 随机森林解决回归问题，GBDT解决分类问题
④ 这两种方法都可以用来解决回归问题
A. ①
B. ②
C. ③
D. ①和④

10. 关于朴素贝叶斯，下列说法错误的是：（D）
A. 它是一个分类算法
B. 朴素的意义在于它的一个天真的假设：所有特征之间是相互独立的
C. 它实际上是将多条件下的条件概率转换成了单一条件下的条件概率，简化了计算
D. 朴素贝叶斯不需要使用联合概率

====多选题====(每题2分)
1. 下列关于Series说法正确的是（ ABCD）。
A.Series是一个类似一维数组的对象
B.Series可以保存任何类型的数据
C.Series由数据和索引构成
D.Series结构的数据位于索引的左侧

2. 下列关于Pandas的索引说法正确的是（ACD ）。
A.Pandas中的索引都是Index类对象
B.索引对象是可修改的
C.索引对象是不可修改的
D.Index对象是可以共享的

3. 线性回归中，我们可以使用正规方程来求解系数，下列关于正规方程的说法正确的是（ABC）
A. 不需要选择学习率 
B. 当特征数量很多时，运算量会增大
C. 不需要迭代训练
D. 随时都可以使用, 不会有问题

4. 关于逻辑回归的损失函数，下列描述正确的是：（ABCD）
A. 它假设样本服从伯努利分布
B. 它的计算使用了自然对数
C. 为了降低损失，必须提升当前类别所属正确分类的概率
D. 它使用了极大似然估计

5. 关于信息增益，决策树分裂节点，下列说法中正确的是（BC）
A. 纯度高的节点需要更多的信息去区分
B. 信息增益可以用“1比特-熵”获得
C. 如果选择一个属性具有多个类别值，那么这个信息增益是有偏差的
D. 选取差异值小的进行划分

6. 假设模型训练时使用的样本类别非常不平衡，主要类别占据了训练数据的99%，现在你的模型在训练集上表现为99%的准确率，那么下面说法正确的是（AC）
A. 准确度并不适合衡量不平衡类别问题
B. 准确率适合衡量不平衡类别问题
C. 精确度和召回率适合于衡量不平衡类别问题
D. 精确度和召回率不适合衡量不平衡类别问题

7. 关于AdaBoost，下列说法中正确的是（AC）
A. 它是一种集成学习算法
B. 每个分类器的权重和被它正确分类的样本的权重相同
C. 后一个基学习器要依赖于前一个基学习器的分类错误率和样本的权重
D. 后一个基学习器每次只学习前一个基学习器被分错的样本

8. 聚类算法研究的问题包括（ABC）
A. 使最终类别分布比较合理
B. 快速聚类
C. 准确度高
D. 能自动识别聚类中心的个数

9. 关于PCA，下列说法中错误的是（多选）：（AD）
A. PCA属于特征选择中的嵌入式降维方法
B. PCA是将原始特征数据的维度尽量压缩，以损失少量信息为代价换来包含主成分信息的低维特征数据
C. 它的内部实现使用了矩阵分解的原理
D. sklearn.decomposition.PCA中的n_components参数为整数时表示降维后被压缩的特征个数

10. 下列有关SVM说法不正确的是：（BC）
A.  SVM使用核函数的过程实质是进行特征转换的过程
B.  SVM对线性不可分的数据也没有较好的分类性能
C.  SVM因为使用了核函数，因此它没有过拟合的风险
D.  SVM的支持向量是少数的几个数据点向量

====简答题=====(30分)
1. 简述KNN算法中K值的不同取值对算法的影响。(8分)
答：
① 若k值过小，训练误差会减小，对应的测试误差会增大，模型有过拟合的风险。
② 若k值过大，训练误差会增大，对应的测试误差会减小，模型会变的相对简单，结果更容易受到异常值的影响。
③ 若k值与训练集样本数相同，会导致最终模型的结果都是指向训练集中类别数最多的那一类，忽略了数据当中其它的重要信息，模型会过于简单。
④ 实际工作中经常使用交叉验证的方式去选取最优的k值，而且一般情况下，k值都是比较小的数值。

阅卷标准
写出一条给2分, 一共8分


2. 简述欠拟合与过拟合产生的原因以及解决方法。(12分)

答：
欠拟合产生的原因：
① 训练的数据量过少；
② 模型过于简单。
 解决办法：
① 增加模型训练的数据量；
② 增加模型的复杂度；
③ 增加多项式特征。

过拟合产生的原因：
① 训练的样本特征过多；
② 模型过于复杂。
解决办法：
① 对原始数据加大数据清洗力度；
② 增加训练的样本数量，直到样本数量远大于特征数；
③ 使用正则化；
④ 对特征进行筛选，减少特征维度；
⑤ 使用集成学习的方法来构建模型。

阅卷标准:
每写出一条1分, 一共12分

3. 请简述HMM的两个基本假设分别是什么。(10分)

答：
① 齐次马尔科夫假设：HMM模型中任意时刻的隐藏状态只依赖于它的前一个隐藏状态。
② 观测独立性假设：HMM模型中任意时刻的观测状态仅仅只依赖于当前时刻的隐藏状态。
阅卷标准
写出一条5分, 一共10分

    
====编程题=====(30分)
1. 现有如下数据：
    [[22, 18, 4, 35, 47, 11],
    [61, 33, 20, 48, 10, 25],
    [55, 74, 29, 81, 16, 27],
    [123, 54, 73, 21, 99, 83],
    [6, 55, 79, 33, 24, 63]]
请尝试使用PCA对如上数据进行降维。
要求：分别使用小数和整数给参数n_components赋值， 并查看对比最终处理的结果。(12分)
参考代码:
from sklearn.decomposition import PCA
data = [[22, 18, 4, 35, 47, 11],
        [61, 33, 20, 48, 10, 25],
        [55, 74, 29, 81, 16, 27],
        [123, 54, 73, 21, 99, 83],
        [6, 55, 79, 33, 24, 63]]

# 实例化PCA, 小数——保留多少信息
transfer = PCA(n_components=0.95)
# 调用fit_transform
data1 = transfer.fit_transform(data)
print("保留95%的信息，降维结果为：\n", data1)

transfer1 = PCA(n_components=0.85)
data2 = transfer1.fit_transform(data)

print("保留85%的信息，降维结果为：\n", data2)

# 实例化PCA, 整数——指定降维到的维数
transfer2 = PCA(n_components=4)
# 调用fit_transform
data3 = transfer2.fit_transform(data)
print("特征数据降维到4维的结果为：\n", data3)

transfer2 = PCA(n_components=5)
data4 = transfer2.fit_transform(data)
print("特征数据降维到5维的结果为：\n", data4)

阅卷标准:
1. 导包2分 即 from sklearn.decomposition import PCA
2. transfer = PCA(n_components=0.95) 这行代码 3分
3. 打印结果2分
4. transfer2 = PCA(n_components=4) 这行代码 3分
5. 打印整数结果2分


2. 使用鸢尾花数据集训练lightGBM分类模型 (18分)
要求：
1. 使用sklearn内置的鸢尾花数据集；
2. 对数据集进行划分，验证集比例可以自定义，保证程序每次使用的数据集都是相同的；
3. 使用合适的特征预处理方法对原始数据进行处理；
4. 使用交叉验证和网格搜索对超参数进行调优(包括但不限于K值)；
5. 评估训练好的模型；
6. 获取表现最好的模型在测试集上的准确率。
7. 获取在交叉验证中表现最好的模型及其参数。

提示:
api函数: LGBMClassifier(boosting_type = 'gbdt',objective = 'multiclass', metric="multi_logloss")
下列参数任选两个进行交叉验证和网格搜索
learning_rate                 
max_depth
sub_feature
num_leaves
colsample_bytree
n_estimators
early_stop


参考代码:
# -*- coding: utf-8 -*-
# @Author  : Chinesejun
# @Email   : itcast@163.com
# @File    : demo.py
# @Software: PyCharm


from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import lightgbm as lgb

# 1.获取鸢尾花数据
iris = load_iris()

# 2.数据集划分
iris = load_iris()
data = iris.data
target = iris.target
x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2)

# 3.特征预处理
transfer = StandardScaler()
x_train = transfer.fit_transform(x_train)
x_test = transfer.transform(x_test)

# 4. 实例化一个估计器
estimator = lgb.LGBMClassifier(boosting_type = 'gbdt',objective = 'multiclass', metric="multi_logloss")

# 5 交叉验证,网格搜索
param_grid = {"learning_rate": [0.01, 0.02, 0.1], "max_depth": [1, 3, 5, 7]}
estimator = GridSearchCV(estimator, param_grid=param_grid, cv=5)

# 6. 模型的训练及调优
estimator.fit(x_train, y_train)

# 7. 得出预测值
y_pre = estimator.predict(x_test)
print("预测值是:\n", y_pre)

# 8. 计算模型的准确率
score = estimator.score(x_test, y_test)
print("准确率为:\n", score)

# 9. 得到交叉验证中表现最好的模型的准确率及其参数
print("在交叉验证中,得到的最好结果是:\n", estimator.best_score_)
print("在交叉验证中,得到的最好的模型是:\n", estimator.best_estimator_)

阅卷标准:
每步2分, 一共九步, 一共18分


