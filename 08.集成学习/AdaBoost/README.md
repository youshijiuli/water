Adaboost[Freund and Schapire, 1997]是一种boosting算法。与boosting算法相似的有bagging，两者同属于ensemble learning。

把准确率不高但在50%以上(比如60%)的分类器称为弱分类器，弱分类器可以是一些基础的ML算法比如单层决策树等。对于同一数据可能有多种弱分类器，adaboost就是将这些弱分类器结合起来，称为基分类器，从而有效提高整体的准确率。但是要想获得好的结合或者集成，对基分类器的要求是“好而不同”，即有一定的准确率，而且弱分类器之间要有“多样性”，比如一个判断是否为男性的任务，弱分类器1侧重从鼻子、耳朵这些特征判断是否是男人，分类器2侧重脸和眼睛等等，把这些分类器结合起来就有了所有用来判断是否男性的特征，并且adaboost还可以给每个基分类器赋值不同的权重，比如从脸比鼻子更能判断是否为男性，就可以把分类器2的权重调高一些，这也是adaboost需要学习的内容。

## 数学推导

AdaBoost可以表示为基分类器的线性组合：
$$
H(\boldsymbol{x})=\sum_{i=1}^{N} \alpha_{i} h_{i}(\boldsymbol{x})
$$
其中$h_i(x),i=1,2,...$表示基分类器，$\alpha_i$是每个基分类器对应的权重，表示如下：
$$
\alpha_{i}=\frac{1}{2} \ln \left(\frac{1-\epsilon_{i}}{\epsilon_{i}}\right)
$$
其中$\epsilon_{i}$是每个弱分类器的错误率。



