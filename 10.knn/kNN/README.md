
kNN(k-Nearest Neighbor)既可以分类，也可以用于回归。其工作机制比较简单：给定测试样本，找出训练集中与其距离最靠近的$k$个训练样本，称为Neighbor然后基于这$k$个Neighbor的信息来预测。通常在分类任务中，使用投票法，即选择这$k$个样本中出现最多的label作为预测结果；而在回归任务中，可使用平均法，即将这$k$个样本中的label值(在回归任务中应该是输出值，这里为了方便也叫label)平均作为预测结果，还可以基于距离远近进行加权平均投票，距离越近的权重越大。

kNN中需要考虑两点：

* $k$是一个重要参数，$k$不同分类结果会有显著不同。$k$值的选择可简单参考[锚点](#anchor)[KNN中的K怎么选择](https://zhuanlan.zhihu.com/p/30425907)，如需严格深入须参考相关书籍或论文
* 距离的计算方式，通常采用欧式距离，也有马氏距离等


## 伪代码

> 抽空重写

对未知类别属性的数据集中的每个点依次执行以下操作：
(1) 计算已知类别数据集中的点与当前点之间的距离；
(2) 按照距离递增次序排序；
(3) 选取与当前点距离最小的k个点(k<20)；
(4) 确定前k个点所在类别的出现频率；
(5) 返回前k个点出现频率最高的类别作为当前点的预测分类。

## 优缺点

基本参考《机器学习实战》，其中称型目标变量的结果只在有限目标集中取值，如真与假、动物分类集合{ 爬行类、鱼类、哺乳类、两栖类} ；数值型目标变量则可以从无限的数值集合中取值，如0.100、42.001、1000.743 等。数值型目标变量主要用于回归分析。

* 优点
  
  精度高、对异常值不敏感、无数据输入假定

* 缺点

  计算复杂度高、空间复杂度高。

* 适用数据
  
  数值型和标称型

## $k$的选择



## Refs

西瓜书

机器学习实战

[李航](https://github.com/SmirkCao/Lihang/tree/master/CH03)







